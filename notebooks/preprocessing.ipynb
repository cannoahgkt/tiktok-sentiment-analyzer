{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56950fc0",
   "metadata": {},
   "source": [
    "# TikTok Sentiment Analysis - Data Preprocessing\n",
    "\n",
    "This notebook contains the complete preprocessing pipeline for TikTok comment sentiment analysis.\n",
    "We'll cover data loading, cleaning, exploratory data analysis, and prepare data for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770c60e",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import necessary libraries for data manipulation, text processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c8098",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration\n",
    "\n",
    "Load the TikTok comments data and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comments data\n",
    "df = pd.read_csv('../data/comments.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1bc27",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions\n",
    "\n",
    "Define functions for cleaning and preprocessing text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b35ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809e92a",
   "metadata": {},
   "source": [
    "## Apply Text Preprocessing\n",
    "\n",
    "Clean the comment text and create processed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning\n",
    "df['cleaned_comment'] = df['comment_text'].apply(clean_text)\n",
    "\n",
    "# Remove empty comments after cleaning\n",
    "df = df[df['cleaned_comment'].str.len() > 0]\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
    "print(\"\\nSample cleaned comments:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df.iloc[i]['comment_text']}\")\n",
    "    print(f\"Cleaned: {df.iloc[i]['cleaned_comment']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274161d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with TextBlob\n",
    "\n",
    "Perform initial sentiment analysis using TextBlob to create labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06693b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Get sentiment polarity and subjectivity using TextBlob\n",
    "    \"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "def classify_sentiment(polarity):\n",
    "    \"\"\"\n",
    "    Classify sentiment based on polarity score\n",
    "    \"\"\"\n",
    "    if polarity > 0.1:\n",
    "        return 'positive'\n",
    "    elif polarity < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_data = df['comment_text'].apply(get_sentiment)\n",
    "df['polarity'] = [x[0] for x in sentiment_data]\n",
    "df['subjectivity'] = [x[1] for x in sentiment_data]\n",
    "df['sentiment'] = df['polarity'].apply(classify_sentiment)\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(df['sentiment'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5c221",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Visualize the data and sentiment distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359015f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting environment\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Sentiment Distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Sentiment Distribution')\n",
    "\n",
    "# 2. Polarity Distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(df['polarity'], bins=20, alpha=0.7, color='skyblue')\n",
    "plt.title('Polarity Score Distribution')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 3. Subjectivity Distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(df['subjectivity'], bins=20, alpha=0.7, color='lightcoral')\n",
    "plt.title('Subjectivity Score Distribution')\n",
    "plt.xlabel('Subjectivity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 4. Comment Length Distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "df['comment_length'] = df['comment_text'].str.len()\n",
    "plt.hist(df['comment_length'], bins=20, alpha=0.7, color='lightgreen')\n",
    "plt.title('Comment Length Distribution')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 5. Sentiment vs Length\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.boxplot(data=df, x='sentiment', y='comment_length')\n",
    "plt.title('Comment Length by Sentiment')\n",
    "\n",
    "# 6. Polarity vs Subjectivity\n",
    "plt.subplot(2, 3, 6)\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    subset = df[df['sentiment'] == sentiment]\n",
    "    plt.scatter(subset['polarity'], subset['subjectivity'], \n",
    "               c=colors[sentiment], label=sentiment, alpha=0.6)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.title('Polarity vs Subjectivity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4011be",
   "metadata": {},
   "source": [
    "## Word Cloud Analysis\n",
    "\n",
    "Generate word clouds for different sentiment categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ca4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds for each sentiment\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sentiments = ['positive', 'negative', 'neutral']\n",
    "colors = ['Greens', 'Reds', 'Blues']\n",
    "\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    text = ' '.join(df[df['sentiment'] == sentiment]['cleaned_comment'])\n",
    "    \n",
    "    if text.strip():  # Only create wordcloud if there's text\n",
    "        wordcloud = WordCloud(width=400, height=300, \n",
    "                            background_color='white',\n",
    "                            colormap=colors[i]).generate(text)\n",
    "        \n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(f'{sentiment.title()} Comments', fontsize=16)\n",
    "        axes[i].axis('off')\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, f'No {sentiment} comments', \n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "        axes[i].set_title(f'{sentiment.title()} Comments', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d71df7",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Create additional features that might be useful for sentiment prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5358149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "df['word_count'] = df['cleaned_comment'].str.split().str.len()\n",
    "df['exclamation_count'] = df['comment_text'].str.count('!')\n",
    "df['question_count'] = df['comment_text'].str.count('\\?')\n",
    "df['caps_ratio'] = df['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "# Show feature correlations with sentiment\n",
    "feature_cols = ['word_count', 'exclamation_count', 'question_count', 'caps_ratio', 'polarity', 'subjectivity']\n",
    "correlation_data = df[feature_cols + ['sentiment']].copy()\n",
    "\n",
    "# Convert sentiment to numeric for correlation\n",
    "sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "correlation_data['sentiment_numeric'] = correlation_data['sentiment'].map(sentiment_map)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = correlation_data.drop('sentiment', axis=1).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature statistics by sentiment:\")\n",
    "print(df.groupby('sentiment')[feature_cols].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6880c1",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data\n",
    "\n",
    "Save the cleaned and processed dataset for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed dataset\n",
    "output_path = '../data/preprocessed_comments.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to: {output_path}\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(\"\\nFinal dataset sample:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75fc2b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This preprocessing notebook has completed the following steps:\n",
    "\n",
    "1. ✅ Data loading and exploration\n",
    "2. ✅ Text cleaning and preprocessing\n",
    "3. ✅ Initial sentiment analysis with TextBlob\n",
    "4. ✅ Exploratory data analysis and visualization\n",
    "5. ✅ Feature engineering\n",
    "6. ✅ Data export for modeling\n",
    "\n",
    "**Next steps for the project:**\n",
    "- Create a machine learning model for sentiment classification\n",
    "- Implement the TikTok comment fetcher\n",
    "- Build the Streamlit web application\n",
    "- Deploy the application\n",
    "\n",
    "The preprocessed data is now ready for machine learning model training!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
